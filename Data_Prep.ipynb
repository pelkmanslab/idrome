{
 "metadata": {
  "name": "",
  "signature": "sha256:82a05e0c5c498ad0a751191a96e01c51af5f8788ff6f09e1701ec036f9ffd4dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook retrieves and prepares the data for analysis. The **`Cell->Run all`** menu option will fully reconstruct all the data used in this project. Two new directories will be created: `human/` and `viral/`, which will contain the information for their namesake proteome(s). \n",
      "\n",
      "Note: This takes about 30 minutes on my machine due to running the disorder algorithms (CAST and IUpred) sequentially and loading the full dataset twice into memory, see **`mtRunner.py`** for a high perf version.\n",
      "\n",
      "#### Sections:\n",
      "\n",
      "- [Human proteome](#Human-proteome)\n",
      "- [Human virus proteomes](#Human-virus-proteomes)\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from __future__ import print_function\n",
      "import json, glob, os, re, ast"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Human proteome"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This uses my simple [pyuniprot](https://github.com/tlnagy/pyuniprot) script to download the latest release from Uniprot for the complete reviewed human proteome and saves the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request = {\n",
      "    'columns':'id,organism-id,protein names,length,go,go-id,taxon,feature(CHAIN),feature(DOMAIN EXTENT),feature(REGION),feature(COMPOSITIONAL BIAS),sequence,keywords,genes',\n",
      "    'output':'human/gw_human_proteome',\n",
      "    'query':'reviewed:yes AND organism:\"Homo sapiens (Human) [9606]\"',\n",
      "    'colnames':'ACCID,ORGID,PROTNAME,LENGTH,GO,GOID,TAXON,CHAIN,DOMFT,REGION,COMPBIAS,SEQ,KEYWORDS,GENENAME'\n",
      "}\n",
      "json.dump(request, open('tmpjson.txt', 'w'))\n",
      "!mkdir -p human\n",
      "!cd pyuniprot/ && cat ../tmpjson.txt | ./pyuniprot.py > ../pyuniprot_output.txt && cd ..\n",
      "with open('pyuniprot_output.txt', 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        if '.csv' in line:\n",
      "            file_name = line.rsplit('../')[-1]\n",
      "!rm -f pyuniprot_output.txt\n",
      "!rm -f tmpjson.txt\n",
      "config = json.load(open(\"config.json\"))\n",
      "config['rawdata']['human'] = file_name.strip()\n",
      "json.dump(config, open('config.json', 'w'), indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Running CAST and IUPred"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The current version runs sequentially so its performance is quite poor. It also has both the input and the output from the algorithms in memory at the same time so expect to see high memory pressure.  **`multithreading.py`** has parallelized version that is much faster, but it's not yet feature-complete."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DisorderedAlgoRunner import *\n",
      "config = json.load(open(\"config.json\"))\n",
      "runDisorderedAnalysis(config['rawdata']['human'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No cast output found. Generating...\n",
        "Hold tight this can take awhile if the dataset is big...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DisorderedAlgoRunner.py:121: UserWarning: Running cast sequentially on >10000 sequences takes awhile. Memory pressure will also be quite high because the entire input and output datasets are in memory simultaneously\n",
        "  \" will also be quite high because the entire input and output datasets are in memory simultaneously\")\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running CAST on 20194 sequences...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CAST run complete. Loading file and processing...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No iupred output found. Generating...\n",
        "Hold tight this can take awhile if the dataset is big...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/tamasnagy/anaconda/envs/idrome/lib/python2.7/site-packages/pandas/io/parsers.py:1139: DtypeWarning: Columns (101,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
        "  data = self._reader.read(nrows)\n",
        "DisorderedAlgoRunner.py:77: UserWarning: Running IUPred sequentially on >5000 sequences takes awhile. Memory pressure will also be quite high because the entire input and output datasets are in memory simultaneously\n",
        "  \" will also be quite high because the entire input and output datasets are in memory simultaneously\")\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running IUPRED on 20194 sequences...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IUPred run complete. Loading file and running thresholding...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Writing results to file.\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the csv file using the [pandas](http://pandas.pydata.org/pandas-docs/stable/) library."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Human virus proteomes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The viral proteomes used here are defined as the ones that have complete proteomes that are reviewed and have humans as their hosts. The viral proteomes are a lot less straightforward than the human one. As a result, we'll have to do quite a bit of post-processing. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "request = {\n",
      "    'query':'taxonomy:\"Viruses [10239]\" AND \"complete proteome\" AND reviewed:yes AND host:\"Homo sapiens (Human) [9606]\"',\n",
      "    'columns':'id,organism-id,protein names,length,go,go-id,taxon,feature(CHAIN),feature(DOMAIN EXTENT),feature(REGION),feature(COMPOSITIONAL BIAS),sequence,keywords,genes',\n",
      "    'output':'viral/human_viral_proteome',\n",
      "    'colnames':'ACCID,ORGID,PROTNAME,LENGTH,GO,GOID,TAXON,CHAIN,DOMFT,REGION,COMPBIAS,SEQ,KEYWORDS,GENENAME'\n",
      "}\n",
      "json.dump(request, open('tmpjson.txt', 'w'))\n",
      "!mkdir -p viral\n",
      "!cd pyuniprot/ && cat ../tmpjson.txt | ./pyuniprot.py > ../pyuniprot_output.txt && cd ..\n",
      "with open('pyuniprot_output.txt', 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        if '.csv' in line:\n",
      "            file_name = line.rsplit('../')[-1]\n",
      "!rm -f pyuniprot_output.txt\n",
      "!rm -f tmpjson.txt\n",
      "config = json.load(open(\"config.json\"))\n",
      "config['rawdata']['viral'] = file_name.strip()\n",
      "json.dump(config, open('config.json', 'w'), indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sanitizing input\n",
      "\n",
      "A couple things need to be done before proceeding with the data:\n",
      "\n",
      "- Remove any unclassified or unassigned proteins\n",
      "- Split polyproteins into their cleaved products based on the CHAIN information\n",
      "- Filter out any pre-cursor proteins (risky)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Retrieve the spreadsheet of protein data\n",
      "input_name = config['rawdata']['viral']\n",
      "\n",
      "df = pd.read_csv(input_name)\n",
      "if np.count_nonzero(~pd.isnull(df.tail(1))) is 0:\n",
      "    # Add 1 since python uses 0-based indexing\n",
      "    print(\"Dropping terminal null value on line %s\\n\"%(df.tail(1).index.values[0]+1))\n",
      "    df.drop(df.tail(1).index, inplace=True)\n",
      "\n",
      "unclassifed = df[df.TAXON.str.contains('unclassified|unassigned')]\n",
      "print(\"%s proteins contain an unclassifed or an unassigned taxonomic level. Dropping...\"%len(unclassifed))\n",
      "for prot in unclassifed['TAXON']: print(\"       --> %s \"%prot[:50])\n",
      "print('')\n",
      "df.drop(unclassifed.index, inplace=True)\n",
      "\n",
      "# Proteins that are cleaved into more than one part as part of their PTMs\n",
      "ptmed_prots = df[df['CHAIN'].str.count('CHAIN') > 1]\n",
      "\n",
      "# Remove these from the protein list, we'll re-add them and their cleavage products\n",
      "df = df[~df['ACCID'].isin(ptmed_prots['ACCID'])].reset_index(drop=True)\n",
      "\n",
      "print(\"Percent proteins that are cleaved during PTM: %s%%\"%(len(ptmed_prots['PROTNAME'])/float(len(df))*100))\n",
      "\n",
      "# Create a list of the polyproteins and their cleavage products\n",
      "f = open('%s_%s_addedpolyprots.txt'%(input_name.rsplit('/', 1)[0]+\"/\" if '/' in input_name else '', \n",
      "                                     input_name.rsplit('.csv', 1)[0].rsplit('/', 1)[-1]), 'wb')\n",
      "\n",
      "proteins_to_add = []\n",
      "print('\\nIgnoring the following precursor proteins:')\n",
      "\n",
      "chain_specific_columns = ['REGION', 'DOMFT', 'COMPBIAS']\n",
      "\n",
      "for index, row in ptmed_prots.iterrows():\n",
      "    f.write(\"\\n%s\\n%s\\n\"%(row['TAXON'].split(',')[0].split('(')[0], row['PROTNAME'].split(' [')[0]))\n",
      "    \n",
      "    columns_to_regions = {}\n",
      "    for column_name in chain_specific_columns:\n",
      "        if not pd.isnull(row[column_name]):\n",
      "            columns_to_regions[column_name] = re.findall(r'(\\d+)\\s(\\d+)\\s([^\\.;]*)', row[column_name])\n",
      "    \n",
      "    idx = 1\n",
      "    # Parse all of the separate mature protein chains\n",
      "    for chain in re.findall(r'(\\d+)\\s(\\d+)\\s([^\\.;]*).*?[/]FTId=([^\\.]*)', row['CHAIN']):\n",
      "        # Filter out known precursors\n",
      "        if any(x in chain[2].lower() for x in ['pre-', 'precursor', 'genome polyprotein']):\n",
      "            print(\"       --> %s \"%chain[2])\n",
      "            continue\n",
      "        start = int(chain[0])\n",
      "        end = int(chain[1])\n",
      "        # Leave the potential descriptor in for the time being\n",
      "        name = chain[2].replace(' (By similarity)', '')\n",
      "        \n",
      "        new_row = row.to_dict()\n",
      "\n",
      "        new_row.update({'SEQ':(row['SEQ'].replace(' ', '')[start-1:end]), 'LENGTH': (end-start+1), \n",
      "                        'ACCID':'%s~%s'%(row['ACCID'], idx), 'PROTNAME':name})\n",
      "        for column,regions in columns_to_regions.iteritems():\n",
      "            new_row.update({column:' '.join(['PPDERV %s %s %s.;'%(region) for region \n",
      "                                             in regions if int(region[0]) >= start and int(region[1]) <= end])})\n",
      "        \n",
      "        f.write(\"%s - %s %s\\n\"%(start, end, name))\n",
      "        \n",
      "        proteins_to_add.append(new_row)\n",
      "        idx += 1\n",
      "        \n",
      "f.close()\n",
      "df = df.append(proteins_to_add, ignore_index=True)\n",
      "polyprot_file = \"%s_withpolyprots.csv\"%input_name.rsplit('.csv', 1)[0]\n",
      "config['processed']['viral']['polyprots'] = polyprot_file\n",
      "json.dump(config, open('config.json', 'w'), indent=4)\n",
      "df.to_csv(polyprot_file, index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dropping terminal null value on line 4013\n",
        "\n",
        "0 proteins contain an unclassifed or an unassigned taxonomic level. Dropping...\n",
        "\n",
        "Percent proteins that are cleaved during PTM: 14.6941109205%\n",
        "\n",
        "Ignoring the following precursor proteins:\n",
        "       --> Virion membrane protein A17 precursor \n",
        "       --> Virion membrane protein A17 precursor \n",
        "       --> Pre-capsid vertex protein \n",
        "       --> Pre-capsid vertex protein \n",
        "       --> Pre-capsid vertex protein \n",
        "       --> Pre-protein VI \n",
        "       --> Pre-protein VI \n",
        "       --> Pre-protein VI \n",
        "       --> Capsid protein precursor \n",
        "       --> Pre-early 3 receptor internalization and degradation alpha protein \n",
        "       --> Pre-glycoprotein polyprotein GP complex "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       --> Pre-glycoprotein polyprotein GP complex \n",
        "       --> Pre-glycoprotein polyprotein GP complex \n",
        "       --> Pre-glycoprotein polyprotein GP complex \n",
        "       --> Pre-glycoprotein polyprotein GP complex \n",
        "       --> Pre-glycoprotein polyprotein GP complex \n",
        "       --> Pre-histone-like nucleoprotein \n",
        "       --> Pre-histone-like nucleoprotein \n",
        "       --> Major core protein 4a precursor \n",
        "       --> Major core protein 4a precursor \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein (By similarity) \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Genome polyprotein \n",
        "       --> Pre-small/secreted glycoprotein (By similarity) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       --> Pre-small/secreted glycoprotein (By similarity) \n",
        "       --> Pre-small/secreted glycoprotein (By similarity) \n",
        "       --> Pre-small/secreted glycoprotein (By similarity) \n",
        "       --> Pre-small/secreted glycoprotein \n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "runDisorderedAnalysis(config['processed']['viral']['polyprots'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No cast output found. Generating...\n",
        "Hold tight this can take awhile if the dataset is big...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running CAST on 6976 sequences...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CAST run complete. Loading file and processing...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No iupred output found. Generating...\n",
        "Hold tight this can take awhile if the dataset is big...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DisorderedAlgoRunner.py:77: UserWarning: Running IUPred sequentially on >5000 sequences takes awhile. Memory pressure will also be quite high because the entire input and output datasets are in memory simultaneously\n",
        "  \" will also be quite high because the entire input and output datasets are in memory simultaneously\")\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running IUPRED on 6976 sequences...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IUPred run complete. Loading file and running thresholding...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Writing results to file.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See previously mentioned [caveats](#Running-CAST-and-IUPred) for the disordered analysis."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}